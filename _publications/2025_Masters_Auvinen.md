---
layout: publication # do not change

#### these fields are mandatory. please fill them out
title: "Utilizing parameter-efficient fine-tuning methods to improve controllable text generation" # title of your publication 

# choose one of the following types:
# "paper": Peer-Reviewed Journal and Conference Papers
# "preprint": Preprint
# "thesis": Thesis (e.g. Master/PhD Thesis)
type: thesis
abstract: "Language models have become a crucial component in the field of natural language processing (NLP). Despite demonstrating remarkable performance in various tasks, the text generated by these models often lacks accurate control. This lack of control can lead to social and ethical issues, and can be harmful in applications that require guideline adherence, stylistic personalization or content moderation. This has led to a growing interest in controllable text generation, specifically in multi-aspect controllable text generation (MCTG). Various MCTG methods have been proposed to improve the controllability of language models while maintaining the level of fluency of the generated text. This thesis investigates augmenting an existing method, Disentangled Controllable Generation (DCG), with parameter-efficient fine-tuning (PEFT) methods to improve the controllability of GPT-2. The main experiments consist of comparing four DCG + PEFT variants, namely bottleneck adapters, IA3, LoRA and SSF, with eight baseline methods. The experiments are conducted on two datasets that measure performance in terms of different attribute spaces - YELP contains three attributes (sentiment, pronoun and tense) and Mixture contains two attributes (sentiment and topic). The approaches are also evaluated under two settings called protocols - Original (all attributes seen during training) and Few-Shot (subset of attributes seen during training to measure performance on novel combinations). This is an important consideration since training on a subset of all attribute combinations and generalizing to novel unseen combinations circumvents the need for collecting training data for every attribute combination. The results show that all four PEFT-augmented variations of DCG improve the standalone DCG’s performance in MCTG, with the bottleneck adapter approach attaining the best overall results among all baseline methods. This method improves standalone DCG’s average accuracy from 75.47% to 79.21% (3.74 percentage point improvement), and reduces the average perplexity from 70.44 to 46.87 (23.57 point improvement). The proposed bottleneck adapter approach attains the best overall score among all baseline methods as measured by accuracy-perplexity score (aps), a composite metric that combines accuracy and perplexity. Further analysis reveals that reducing the parameters of the bottleneck adapter-augmented DCG method by 75% still results in improving DCG’s average accuracy and average perplexity by 3.50 and 16.82 absolute points, respectively. A closer examination of this method shows that as the dropout rate of the bottleneck adapter is decreased, slight improvements in average accuracy come at the cost of substantial increases in average perplexity. This highlights the importance of regularizing the method to combat overfitting on sample-specific features."
####


# set this url, if your paper is on another server; defaults to data.jku-vds-lab.at
paper_content_url: 
# uncomment the "hide" property, if you do not want the publication to be displayed on the website (usually you don't need this)
# hide: True
# uncomment the "non_group_project" property, if you only want the publication to be displayed on your personal page (i.e. publications where you contributed, but does not have anything to do with the Vis Group e.g. Master Thesis,...)
# non_group_project: True

institution: Johannes Kepler University Linz 
thesis_type: Master's Thesis
advisors : 
- schedl
- masoudian


#### the following fields are optional, but it is recommended to enter as much information as possible
# The shortname is used for auto-generated titels. e.g. ConfusionFlow
shortname: master-thesis
# add a 2:1 aspect ratio (e.g., width: 400px, height: 200px) to the folder /assets/images/papers/ e.g. 2020_tvcg_confusionflow.png
image: 
# add a 2:1 aspect ratio teaser figure (e.g., width: 1200px, height: 600px) to the folder /assets/images/papers/ e.g. 2020_tvcg_confusionflow_teaser.png
image_large: 

# Authors in the "database" can be used with just the key that is specified in the corresponding .md file (usually it is the lastname in lower case e.g. doe). Authors that do not have an individual page here should be stated with their full name (e.g. John Doe)
# each author is one item in the list. the list is enumerated with dashes ("-")
# e.g:
# authors:
# - doe # .md file exists for this person
# - schedl # .md file exists for this person
# - Max Mustermann # there is no .md file for this person.
authors:
- Hector Auvinen

# when was this publication written/ when was the publication accepted (e.g. 2020)
year: 2025

# if you have an explicit page you want to reference, use this tag; otherwise it will be generated from your doi
publisherurl: https://epub.jku.at/obvulihs/content/titleinfo/11984304
# what is the publication type and other bib specific properties
bibentry: misc
bib:
  journal: # e.g. IEEE Transactions on Visualization and Computer Graphics (to appear)
  editor: 
  publisher: 
  address: 
  doi: 	# e.g.10.1109/TVCG.2020.3012063
  url: 
  volume: 
  number: 
  pages: 
  month:
  school: Johannes Kepler University Linz 

# the name of your publication pdf e.g. 2020_tvcg_confusionflow.pdf; this is usually uploaded to the caleydo aws server
pdf: 
# A supplement PDF e.g. 2017_preprint_taggle_supplement.pdf; this is usually uploaded to the caleydo aws server
supplement: 

# Extra supplements, such as talk slides, data sets, etc.
supplements:
#- name: General UpSet
#  # use link instead of abslink if you want to link to the master directory
#  abslink: http://vials.io/talk/
#  # defaults to a download icon, use this if you want a link-out icon
#  linksym: true

# Link to the repository where the code is hostet
code:

# After the --- you can put information that you want to appear on the website using markdown formatting or HTML. A good example are acknowledgements, extra references, an erratum, etc.
---




